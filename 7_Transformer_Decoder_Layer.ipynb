{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5417292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "plt.style.use(['science', 'notebook', 'grid', 'ieee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493319b",
   "metadata": {},
   "source": [
    "![transformers](transformers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5104b",
   "metadata": {},
   "source": [
    "- Decoder 与 encoder相比, 有两个特殊的 attention sublayers\n",
    "    - masked multi-head (**self**) attention\n",
    "    - encoder-decoder (**cross**) attention\n",
    "        - (k, v) from encoder (also be called memory, from last decoder layer)\n",
    "        - q: decoder input\n",
    "    - They did not share the same weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36a3e5",
   "metadata": {},
   "source": [
    "#### Mask attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda76900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1dc3ada847407d993c4a9aa5534a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec00a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size, config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1aa2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2051, 10029,  2066,  2019,  8612]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "sample_text = 'time flies like an arrow'\n",
    "model_inputs = tokenizer(sample_text, return_tensors='pt', add_special_tokens=False)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5532e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embedding(model_inputs['input_ids'])\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a91b11",
   "metadata": {},
   "source": [
    "### Triangular lower matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f76b8c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[28.1406, -0.3434, -2.2413,  0.5477, -1.5799],\n",
       "          [-0.3434, 28.3550, -1.6914, -0.3065,  0.2185],\n",
       "          [-2.2413, -1.6914, 27.5898,  0.1182, -0.4554],\n",
       "          [ 0.5477, -0.3065,  0.1182, 27.9868,  0.4677],\n",
       "          [-1.5799,  0.2185, -0.4554,  0.4677, 28.8011]]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " torch.Size([1, 5, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.bmm: matrix multiplication of batch-wise\n",
    "import math \n",
    "q = k = v = input_embeddings\n",
    "# (1, 5, 768) * (1, 768, 5) => (1, 5, 5)\n",
    "scores = torch.bmm(q,k.transpose(1,2))/math.sqrt(k.size(-1))\n",
    "scores, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31bfea4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 5]),\n",
       " tensor([[[1., 0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0., 0.],\n",
       "          [1., 1., 1., 0., 0.],\n",
       "          [1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1.]]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = model_inputs['input_ids'].size(-1)\n",
    "# triangular lower matrix, all equals to 1\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "mask.shape, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011596a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28.1406,    -inf,    -inf,    -inf,    -inf],\n",
       "         [-0.3434, 28.3550,    -inf,    -inf,    -inf],\n",
       "         [-2.2413, -1.6914, 27.5898,    -inf,    -inf],\n",
       "         [ 0.5477, -0.3065,  0.1182, 27.9868,    -inf],\n",
       "         [-1.5799,  0.2185, -0.4554,  0.4677, 28.8011]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill(mask == 0, -float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df6c839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28.1406, -0.3434, -2.2413,  0.5477, -1.5799],\n",
       "         [-0.3434, 28.3550, -1.6914, -0.3065,  0.2185],\n",
       "         [-2.2413, -1.6914, 27.5898,  0.1182, -0.4554],\n",
       "         [ 0.5477, -0.3065,  0.1182, 27.9868,  0.4677],\n",
       "         [-1.5799,  0.2185, -0.4554,  0.4677, 28.8011]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a17c814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28.1406,    -inf,    -inf,    -inf,    -inf],\n",
       "         [-0.3434, 28.3550,    -inf,    -inf,    -inf],\n",
       "         [-2.2413, -1.6914, 27.5898,    -inf,    -inf],\n",
       "         [ 0.5477, -0.3065,  0.1182, 27.9868,    -inf],\n",
       "         [-1.5799,  0.2185, -0.4554,  0.4677, 28.8011]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to in place\n",
    "scores.masked_fill_(mask == 0, -float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bad6bf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[28.1406,    -inf,    -inf,    -inf,    -inf],\n",
       "         [-0.3434, 28.3550,    -inf,    -inf,    -inf],\n",
       "         [-2.2413, -1.6914, 27.5898,    -inf,    -inf],\n",
       "         [ 0.5477, -0.3065,  0.1182, 27.9868,    -inf],\n",
       "         [-1.5799,  0.2185, -0.4554,  0.4677, 28.8011]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e83e8",
   "metadata": {},
   "source": [
    "$$\\exp(-\\infty) = 0$$\n",
    "\n",
    "This is for later $softmax (\\cdot)$ operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "716669d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(-float('inf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e26ca4",
   "metadata": {},
   "source": [
    "#### Masked self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9db7a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_attn(q, k, v, mask=None):\n",
    "    dim_k = k.size(-1)\n",
    "    scores = torch.bmm(q, k.transpose(1, 2)/math.sqrt(dim_k))\n",
    "    if mask is not None:\n",
    "        scores.masked_fill_(mask==0, -float('inf'))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    print(attn_weights, attn_weights.shape)\n",
    "    return torch.bmm(attn_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1122204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.4392e-13, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.1079e-13, 1.9200e-13, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.2115e-12, 5.1565e-13, 7.8852e-13, 1.0000e+00, 0.0000e+00],\n",
      "         [6.3932e-14, 3.8617e-13, 1.9682e-13, 4.9541e-13, 1.0000e+00]]],\n",
      "       grad_fn=<SoftmaxBackward0>) torch.Size([1, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2933, -0.3704,  0.2531,  ...,  0.6531,  0.5391,  1.1173],\n",
       "         [ 0.2266, -0.5755, -1.2444,  ...,  0.9320,  0.8994,  0.9623],\n",
       "         [ 0.1204, -1.8380,  1.2742,  ...,  0.4473, -2.7296, -0.7691],\n",
       "         [ 0.6016, -0.0558, -0.6753,  ..., -0.8899, -0.1966,  0.2929],\n",
       "         [-1.4113,  0.5819,  1.2363,  ..., -0.3869, -1.4703, -2.7841]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_attn(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4a4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
